---
title: "R Notebook"
output: html_notebook
---

In this document, I explain my logic for developing a new approach to analyzing the gray whale calf production dataset (Piedras Blancas). The previous approaches (Weller and Perryman 2012, Stewart and Weller 2021) had some implausible assumptions. 

The mean ($\lambda$) of the number of whales passing by the survey area is assumed to be the same within a week. There is no assumption about the mean except it is bounded between 0 and 40 ($ \lambda \sim UNIF(0,40)$). The weekly mean assumption is a bit arbitrary because whales don't care about the day of the week. It appears that the number increases at the beginning of each survey season and reaches a peak, then decreases thereafter. So, it may make sense to make an assumption about means that change daily. 

The true number of whales in the survey area per 3-hr period is considered an independent Poisson random deviate ($n_{true} \sim POI(\lambda)$).  

Furthermore, there should be some auto-correlations between two consecutive observation periods; they are arbitrarily separated by changes in observation shifts. Each shift is up to 3 hr long (1.5 hr x 2 for each observer). It's likely that the number of whales passing by the survey area would be similar between two consecutive shifts than those that are further apart. 

The observed number of whales per shift is assumed to be a binomial random deviate ($n_{obs} \sim BIN(n_{true}, p_{obs})$), where $p_{obs}$ was estimated through a calibration study ($p_{obs} \sim N(0.889, 0.06375^2)$). Looking at the data, there are many zeros. So, it may be better to use some other distributions (e.g., zero-inflated Poisson, zero-inflated negative binomial) to model these data. It's not easy to get zero observations when the detection probability is close to 0.9. There are some other factors affecting the observations. Perhaps, the sighting conditions may be used to model "actual" sighting probability ($ p_{obs}$ = f(sighting condition) and $p_{obs}$ reaches 0.889 as the condition becomes ideal).

I explore the dataset first then develop a new approach that has better assumptions than the previous approaches. 

This needs to get it done from the ground up. 

A script (Extract Excel data.Rmd) was developed to extract raw data.



```{r setup-and-data}

rm(list=ls())
library(jagsUI)
library(tidyverse)
library(lubridate)
library(bayesplot)

#FILES <- list.files(pattern = ".csv$")
data.path <- "data/Processed data v2/"
FILES <- list.files(path = data.path, 
                    pattern = "Processed_by_shift_data")

MCMC.params <- list(n.samples = 500000,
                    n.thin = 100,
                    n.burnin = 300000,
                    n.chains = 3)

n.samples <- MCMC.params$n.chains * ((MCMC.params$n.samples - MCMC.params$n.burnin)/MCMC.params$n.thin)

# get data
col.defs <- cols(Date = col_date(format = "%Y-%m-%d"), 
                 Shift = col_integer(), 
                 Sea_State = col_integer(),
                 Vis = col_integer(),
                 Effort = col_double(),
                 Mother_Calf = col_integer())

count.obs <- effort <- week <- date <- n.obs <- n.weeks <- all.data <- list()
years <- vector(mode = "numeric", length = length(FILES))

i <- 1
for(i in 1:length(FILES)){
  # Sea_State and Vis have -Inf from missing data, which turns into parsing issues
  # These can be ignored.
  data <- read_csv(paste0(data.path, FILES[i]), 
                   col_types = col.defs)

  # 2022 data have a different date format... 
  # if (is.na(data$Date[1])){
  #   col.defs <- cols(Date = col_character(), #(format = "%d-%b-%y"), 
  #                    Shift = col_integer(), 
  #                    Sea_State = col_integer(),
  #                    Vis = col_integer(),
  #                    Mother_Calf = col_integer(),
  #                    Duration = col_double(), 
  #                    Time_T0 = col_double(), 
  #                    Time_0000 = col_double())
  # 
  #   data <- read_csv(paste0(data.path, FILES[i]), 
  #                  col_types = col.defs)
  # 
  # }
  all.dates <- seq.Date(from = min(data$Date), 
                        to = max(data$Date),
                        by = "day") 

  weeks <- seq.Date(from = min(data$Date), 
                    to = max(data$Date) + days(7),
                    by = "week") 

  
  week.vec <- vector(mode = "numeric", length = length(all.dates))
  for (w in 2:length(weeks)){
    week.vec[all.dates < weeks[w] & all.dates >= weeks[w-1]] <- w-1
  }
 
  # Make a template data.frame with filled in dates, shifts, and weeks
  tmp.df.1 <- data.frame(Date = all.dates %>% rep(each = 8),
                         Shift = rep(1:8, times = length(all.dates)),
                         Week = week.vec %>% rep(each = 8),
                         Seq.idx = 1:(length(all.dates)*8))
  
  
  # combine the two data frames to make one. 
  tmp.df.1 %>% 
    left_join(data, by = c("Date", "Shift")) -> all.data[[i]]

}

all.data.2 <- lapply(all.data, FUN = function(x){
  x %>% mutate(Year = year(Date),
               n.hat = (Mother_Calf * (Effort/60))/3) -> tmp
  
  tmp[is.na(tmp)] <- 0
  
  tmp %>% 
    mutate(cumsum.nhat = cumsum(n.hat),
           DOY = (Date - as.Date(paste0(year(Date), "-01-01"))) %>%
             as.numeric()) -> tmp
  return(tmp)
})

count.long.df <- do.call(rbind, all.data.2)

```

Take a look at the data:

```{r plot-cumsum-1990s}
ggplot(count.long.df %>% filter(Year < 2000)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)

```

```{r plot-cumsum-2000-1}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2012)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)



```


```{r plot-cumsum-2000-2}
ggplot(count.long.df %>% filter(Year > 2011, Year < 2023)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)




```


Next, look at how the counts change within each year:


```{r plot-nhat-1990s}
ggplot(count.long.df %>% filter(Year < 2000)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)




```


```{r plot-nhat-2000-1}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2012)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)



```


```{r plot-nhat-2000-2}
ggplot(count.long.df %>% filter(Year > 2011, Year < 2023)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)




```

There are many zeros... so I should look into zero-inflated models. For non-zero values, there are some "trends" in the maximum number of counts per shift, where they increase to the mid-season and then decreases. So, independence of each shift is not defensible. Although... the sighting condition changes rapidly, so statistical autocorrelation probably is not going to tell us about the correlations. Some zeros are real zeros (i.e., no whales) and others are un-observed zeros. These two kinds of zeros need to be modeled separately. 

Using non-zero effort:


```{r plot-nhat-nonzero-effort-1990s}
ggplot(count.long.df %>% filter(Year < 2000, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-nhat-nonzero-effort-2000-1}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2012, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)

```


```{r plot-nhat-nonzer-effort-2000-2}
ggplot(count.long.df %>% filter(Year > 2011, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)


```



```{r plot-n-nonzero-effort-1990s}
ggplot(count.long.df %>% filter(Year < 2000, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-n-nonzero-effort-2000-1}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2012, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)

```

```{r}

ggplot(count.long.df %>% filter(Year > 2011, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)

```

Not surprisingly, more whales are sighted when the effort was longer, although zero counts are made as well. 


From what I have so far, how do DOY and the shift duration affect the number of observed whales. Use GAMs to look at how various factors affect the number of observed whales. 

```{r gam.1, cache=T}
library(mgcv)
library(tidygam)

gam.1 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
             data = count.long.df, 
             family = "tw")

summary(gam.1)
```

```{r}
plot(gam.1)
```


```{r gam.1.1, cache=T}
gam.1.1 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY, Year) + s(Week), 
             data = count.long.df, 
             family = "tw")

summary(gam.1.1)

```


```{r}
plot(gam.1.1)
```


Week doesn't do much so drop it... 

```{r gam.1.2, cache=T}
gam.1.2 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY, Year), 
             data = count.long.df, 
             family = "tw")

summary(gam.1.2)

```

```{r}
plot(gam.1.2)
```


```{r gam.1.3, cache=T}
count.long.df %>% mutate(Year.f = factor(Year)) -> count.long.df

gam.1.3 <- gam(log(Mother_Calf+1) ~ Year.f + s(Effort) + s(DOY, by = Year.f), 
             data = count.long.df, 
             family = "tw")

summary(gam.1.3)

```

```{r}
plot(gam.1.3)
```

Not much variation in DOY among years. 

<!-- Is it related to ENSO events? Extract MEI.v2 values (https://psl.noaa.gov/enso/mei/) -->

<!-- ```{r} -->
<!-- MEI.data <- read.delim(file = "~/Oceans and Maps/Ocenographic data/meiv2_2022-05-18.txt", header = F, -->
<!--                        sep="", -->
<!--                        col.names = c("Year", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) %>% -->
<!--   pivot_longer(cols = "Jan":"Dec", names_to = "Month", values_to = "MEI") -->

<!-- MEI.data %>% filter(Year %in% years) %>% -->
<!--   group_by(Year) %>% -->
<!--   summarize(MEI.yr = sum(MEI)) -> MEI.data.yr -->

<!-- MEI.data %>%  -->
<!--   filter(Year %in% (years-1)) %>% -->
<!--   group_by(Year) %>% -->
<!--   summarize(MEI.yr.1 = sum(MEI)) %>% -->
<!--   mutate(Year = as.numeric(Year) + 1) -> MEI.data.yr.lag.1 -->

<!-- count.long.df %>% left_join(MEI.data.yr, by = "Year") -> count.long.df -->
<!-- count.long.df %>% left_join(MEI.data.yr.lag.1, by = "Year") -> count.long.df   -->


<!-- gam.2 <- gam(Count ~ s(Effort) + s(DOY) + s(Year) +  -->
<!--                s(Week) + s(MEI.yr) + s(MEI.yr.1),  -->
<!--              data = count.long.df) -->
<!-- ``` -->

<!-- That was a bit of digression. They don't add much to explaining deviance. Don't bother it for now. Also, this won't help with filling in the missing data points. -->


```{r gam.2, cache=T}
gam.2 <- gam(Mother_Calf ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
             data = count.long.df, family = "ziP")

summary(gam.2)
```


```{r}
plot_smooths(model = gam.2, 
             series = Year, 
             transform = exp)
```

Tweedie seems to fit better. 


```{r gam.3, cache=T}
gam.3 <- gam(Mother_Calf ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
             data = count.long.df, family = "nb")

summary(gam.3)
```

Negative binomial seems to fit even better. >77% of deviance explained. 


```{r}
plot_smooths(model = gam.3,
             series = Effort,
             transform = exp) 
             
```


```{r}
plot_smooths(model = gam.3,
             series = DOY, transform = exp)
```



```{r}
plot_smooths(model = gam.3,
             series = Year, transform = exp)
```




```{r}
plot_smooths(model = gam.3,
             series = Week, transform = exp)
```




```{r gam.3.1, cache=T}
gam.3.1 <- gam(Mother_Calf ~ s(Effort) + s(DOY, Year) + s(Week), 
             data = count.long.df, family = "nb")

summary(gam.3.1)
```



```{r gam.3.2, cache=T}
gam.3.2 <- gam(Mother_Calf ~ Year.f + s(Effort) + s(DOY, by = Year.f) + s(Week), 
             data = count.long.df, 
             family = "nb")

summary(gam.3.2)
```


```{r gam.4, cache=T}
gam.4 <- gam((Mother_Calf+1) ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
             data = count.long.df, family = "poisson")

summary(gam.4)
```

Negative binomial seems to be the best. 

```{r}
plot_smooths(model = gam.3,
             series = Effort,
             transform = exp)

```


```{r}
plot_smooths(model = gam.3,
             series = DOY,
             transform = exp)

```



```{r}
plot_smooths(model = gam.3,
             series = Week,
             transform = exp)

```


```{r}
plot_smooths(model = gam.3,
             series = Year, 
             series_length = 40,
             transform = exp)

```

The cyclical nature of the year effect is interesting... 

Effects of DOY, Year, and Week seem to be quite small... 

prediction using the gam model
```{r}
pred.data <- count.long.df %>% 
  mutate(Effort = 3)

gam.3.2.pred <- predict.gam(gam.3.2,
                          type = "response",
                          se.fit = T)

pred.data %>% 
  mutate(prediction = gam.3.2.pred$fit,
         LCL = gam.3.2.pred$fit - gam.3.2.pred$se.fit * 1.96,
         UCL = gam.3.2.pred$fit + gam.3.2.pred$se.fit * 1.96) -> pred.data

ggplot(data = pred.data) +
  geom_point(aes(x = Mother_Calf, 
                 y = prediction)) +
  geom_errorbar(aes(x = Mother_Calf, ymin = LCL, ymax = UCL))
```


It does pretty good job for counts that are less than 5, but larger values fail pretty badly. The model fit is okay just because of that. But, the larger values are as important, even though they don't happen too often... 

I need to build a year-specific model to fill in the gaps. 


