---
title: "R Notebook"
output: html_notebook
---

In this document, I explain my logic for developing a new approach to analyzing the gray whale calf production dataset (Piedras Blancas). The previous approaches (Weller and Perryman 2012, Stewart and Weller 2021) had some implausible assumptions. 

The mean ($\lambda$) of the number of whales passing by the survey area is assumed to be the same within a week. There is no assumption about the mean except it is bounded between 0 and 40 ($ \lambda \sim UNIF(0,40)$). The weekly mean assumption is a bit arbitrary because whales don't care about the day of the week. It appears that the number increases at the beginning of each survey season and reaches a peak, then decreases thereafter. So, it may make sense to make an assumption about means that change daily. 

The true number of whales in the survey area per 3-hr period is considered an independent Poisson random deviate ($n_{true} \sim POI(\lambda)$).  

Furthermore, there should be some auto-correlations between two consecutive observation periods; they are arbitrarily separated by changes in observation shifts. Each shift is up to 3 hr long (1.5 hr x 2 for each observer). It's likely that the number of whales passing by the survey area would be similar between two consecutive shifts than those that are further apart. 

The observed number of whales per shift is assumed to be a binomial random deviate ($n_{obs} \sim BIN(n_{true}, p_{obs})$), where $p_{obs}$ was estimated through a calibration study ($p_{obs} \sim N(0.889, 0.06375^2)$). Looking at the data, there are many zeros. So, it may be better to use some other distributions (e.g., zero-inflated Poisson, zero-inflated negative binomial) to model these data. It's not easy to get zero observations when the detection probability is close to 0.9. There are some other factors affecting the observations. Perhaps, the sighting conditions may be used to model "actual" sighting probability ($ p_{obs}$ = f(sighting condition) and $p_{obs}$ reaches 0.889 as the condition becomes ideal).

I explore the dataset first then develop a new approach that has better assumptions than the previous approaches. 

This needs to get it done from the ground up. 

A script (Extract Excel data.Rmd) was developed to extract raw data. There aer some differences in extraction methods (Stewart's and mine) for earlier years. The comparison is done in *compare_data_extractions.Rmd*.


```{r setup-and-data}

rm(list=ls())
library(jagsUI)
library(tidyverse)
library(lubridate)
library(bayesplot)

#FILES <- list.files(pattern = ".csv$")
data.path <- "data/Processed data v2/"
FILES <- list.files(path = data.path, 
                    pattern = "Processed_by_shift_data")

MCMC.params <- list(n.samples = 500000,
                    n.thin = 100,
                    n.burnin = 300000,
                    n.chains = 3)

n.samples <- MCMC.params$n.chains * ((MCMC.params$n.samples - MCMC.params$n.burnin)/MCMC.params$n.thin)

# get data
col.defs <- cols(Date = col_date(format = "%Y-%m-%d"), 
                 Shift = col_integer(), 
                 Sea_State = col_character(), # contains entries like 3/4... this needs to be dealt with eventually
                 Vis = col_character(),
                 Effort = col_double(),
                 Mother_Calf = col_integer())

count.obs <- effort <- week <- date <- n.obs <- n.weeks <- all.data <- list()
years <- vector(mode = "numeric", length = length(FILES))

#options(warn = 2)  # Change warnings to errors
options(warn = 0)
i <- 6
for(i in 1:length(FILES)){
  # Sea_State and Vis have -Inf from missing data, which turn into parsing issues
  # These can be ignored.
  data <- read_csv(paste0(data.path, FILES[i]), 
                   col_types = col.defs)

  # 2022 data have a different date format... 
  # if (is.na(data$Date[1])){
  #   col.defs <- cols(Date = col_character(), #(format = "%d-%b-%y"), 
  #                    Shift = col_integer(), 
  #                    Sea_State = col_integer(),
  #                    Vis = col_integer(),
  #                    Mother_Calf = col_integer(),
  #                    Duration = col_double(), 
  #                    Time_T0 = col_double(), 
  #                    Time_0000 = col_double())
  # 
  #   data <- read_csv(paste0(data.path, FILES[i]), 
  #                  col_types = col.defs)
  # 
  # }
  all.dates <- seq.Date(from = min(data$Date), 
                        to = max(data$Date),
                        by = "day") 

  weeks <- seq.Date(from = min(data$Date), 
                    to = max(data$Date) + days(7),
                    by = "week") 

  
  week.vec <- vector(mode = "numeric", length = length(all.dates))
  for (w in 2:length(weeks)){
    week.vec[all.dates < weeks[w] & all.dates >= weeks[w-1]] <- w-1
  }
 
  # Make a template data.frame with filled in dates, shifts, and weeks
  tmp.df.1 <- data.frame(Date = all.dates %>% rep(each = 8),
                         Shift = rep(1:8, times = length(all.dates)),
                         Week = week.vec %>% rep(each = 8),
                         Seq.idx = 1:(length(all.dates)*8))
  
  
  # combine the two data frames to make one. 
  tmp.df.1 %>% 
    left_join(data, by = c("Date", "Shift")) -> all.data[[i]]

}

all.data.2 <- lapply(all.data, FUN = function(x){
  x %>% mutate(Year = year(Date),
               n.hat = (Mother_Calf * (Effort/60))/3) -> tmp
  
  tmp[is.na(tmp)] <- 0
  
  tmp %>% 
    mutate(cumsum.nhat = cumsum(n.hat),
           DOY = (Date - as.Date(paste0(year(Date), "-01-01"))) %>%
             as.numeric()) -> tmp
  return(tmp)
})

count.long.df <- do.call(rbind, all.data.2)

# Find sea state and visibility with "/"
# Take a larger value.

idx.Fslash.BF <- grep("/", count.long.df$Sea_State)
BF.max <- lapply(count.long.df$Sea_State[idx.Fslash.BF], 
                FUN = function(x) {strsplit(x, "/") %>% 
                    unlist() %>% 
                    as.numeric() %>% 
                    max()}) %>% unlist()

count.long.df$Sea_State[idx.Fslash.BF] <- as.character(BF.max)

idx.Fslash.VS <- grep("/", count.long.df$Vis)
VS.max <- lapply(count.long.df$Sea_State[idx.Fslash.VS], 
                FUN = function(x) {strsplit(x, "/") %>% 
                    unlist() %>% 
                    as.numeric() %>% 
                    max()}) %>% unlist()

count.long.df$Vis[idx.Fslash.VS] <- as.character(VS.max)

```

Take a look at the data:

```{r plot-cumsum-1990s}
ggplot(count.long.df %>% filter(Year < 2000)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)

```

```{r plot-cumsum-2000}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2010)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)



```


```{r plot-cumsum-2010}
ggplot(count.long.df %>% filter(Year > 2009, Year < 2020)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-cumsum-2020}
ggplot(count.long.df %>% filter(Year > 2019)) +
  geom_point(aes(x = DOY, y = cumsum.nhat)) +
  facet_wrap(Year ~., nrow = 3)

```

Next, look at how the counts change within each year:


```{r plot-nhat-1990s}
ggplot(count.long.df %>% filter(Year < 2000)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)




```


```{r plot-nhat-2000}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2010)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)



```


```{r plot-nhat-2010}
ggplot(count.long.df %>% filter(Year > 2009, Year < 2020)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)

```


```{r plot-nhat-2020}
ggplot(count.long.df %>% filter(Year > 2019)) +
  geom_point(aes(x = DOY, y = n.hat)) +
  facet_wrap(Year ~., nrow = 3)

```

There are many zeros... so I should look into zero-inflated models. For non-zero values, there are some "trends" in the maximum number of counts per shift, where they increase to the mid-season and then decreases. So, independence of each shift is not defensible. Although... the sighting condition changes rapidly, so statistical autocorrelation probably is not going to tell us about the correlations. Some zeros are real zeros (i.e., no whales) and others are un-observed zeros. These two kinds of zeros need to be modeled separately. 

Using non-zero effort:


```{r plot-nhat-nonzero-effort-1990s}
ggplot(count.long.df %>% filter(Year < 2000, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-nhat-nonzero-effort-2000}
ggplot(count.long.df %>% filter(Year > 1999, Year < 2010, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)

```


```{r plot-nhat-nonzer-effort-2010}
ggplot(count.long.df %>% filter(Year > 2009, Year < 2020, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-nhat-nonzer-effort-2020}
ggplot(count.long.df %>% filter(Year > 2019, Effort > 0)) +
  geom_point(aes(x = DOY, y = n.hat, color = factor(Shift))) +
  facet_wrap(Year ~., nrow = 3)


```

```{r plot-n-nonzero-effort-1990s}
ggplot(count.long.df %>% 
         filter(Year < 2000, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)


```


```{r plot-n-nonzero-effort-2000}
ggplot(count.long.df %>% 
         filter(Year > 1999, Year < 2010, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)

```



```{r plot-n-nonzero-effort-2010}
ggplot(count.long.df %>% 
         filter(Year > 2009, Year < 2020, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)

```

```{r plot-n-nonzero-effort-2020}

ggplot(count.long.df %>% 
         filter(Year > 2019, Effort > 0)) +
  geom_point(aes(x = Effort, y = n.hat, color = DOY)) +
  facet_wrap(Year ~., nrow = 3)

```

Not surprisingly, more whales are sighted when the effort was longer, although zero counts are made as well. 


From what I have so far, how do DOY and the shift duration affect the number of observed whales. Use GAMs to look at how various factors affect the number of observed whales. 

```{r gam.1, cache=T}
library(mgcv)
library(tidygam) # https://github.com/stefanocoretta/tidygam
#library(tidymv) # replaced by tidygam

count.long.df %>% 
  mutate(BF = as.numeric(Sea_State),
         VS = as.numeric(Vis)) -> count.long.df

if (!file.exists("RData/GAM_1.rds")){
  gam.1 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
               data = count.long.df, 
               family = "tw")
  saveRDS(gam.1, "RData/GAM_1.rds")
} else {
  gam.1 <- readRDS("RData/GAM_1.rds")
}

summary(gam.1)
```

```{r gam.1.pred.effort}
pred.gam.1.effort <- tidygam::predict_gam(model = gam.1,
                                          series = "Effort",
                                          length_out = 200)

plot(pred.gam.1.effort)
```


```{r gam.1.pred.DOY}
pred.gam.1.DOY <- tidygam::predict_gam(model = gam.1,
                                          series = "DOY",
                                          length_out = 200)

plot(pred.gam.1.DOY)
```


```{r gam.1.pred.week}
pred.gam.1.week <- tidygam::predict_gam(model = gam.1,
                                          series = "Week",
                                          length_out = 200)

plot(pred.gam.1.week)
```

```{r gam.1.1, cache=T}
if (!file.exists("RData/GAM_1_1.rds")){
  
  gam.1.1 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY, Year) + s(Week), 
                 data = count.long.df, 
                 family = "tw")
   saveRDS(gam.1.1, "RData/GAM_1_1.rds")
} else {
  gam.1.1 <- readRDS("RData/GAM_1_1.rds")
}
summary(gam.1.1)

```


```{r pred.gam.1.1}
pred.gam.1.1.DOY.year <- tidygam::predict_gam(model = gam.1.1,
                                          series = c("DOY", "Year"),
                                          length_out = 500)

plot(pred.gam.1.1.DOY.year)
#plot(gam.1.1)
```

Subtle difference among years over DOY. Some years go in to later in the season. 



```{r pred.gam.1.1.year}
pred.gam.1.1.year <- tidygam::predict_gam(model = gam.1.1,
                                          series = "Year",
                                          length_out = 200,
                                          tran_fun = exp)

plot(pred.gam.1.1.year)
#plot(gam.1.1)
```

Decline in the last five or so years is apparent here. 


```{r pred.gam.1.1.week}
pred.gam.1.1.week <- tidygam::predict_gam(model = gam.1.1,
                                          series = "Week",
                                          length_out = 200)

plot(pred.gam.1.1.week)
#plot(gam.1.1)
```

Week doesn't do much so drop it... ?

```{r gam.1.2, cache=T}
if (!file.exists("RData/GAM_1_2.rds")){
  gam.1.2 <- gam(log(Mother_Calf+1) ~ s(Effort) + s(DOY, Year), 
                 data = count.long.df, 
                 family = "tw")
   saveRDS(gam.1.2, "RData/GAM_1_2.rds")
} else {
  gam.1.2 <- readRDS("RData/GAM_1_2.rds")
}
summary(gam.1.2)

```



```{r gam.1.3, cache=T}
count.long.df %>% mutate(Year.f = factor(Year)) -> count.long.df

if (!file.exists("RData/GAM_1_3.rds")){
  
  gam.1.3 <- gam(log(Mother_Calf+1) ~ Year.f + s(Effort) + s(DOY, by = Year.f), 
                 data = count.long.df, 
             family = "tw")
  saveRDS(gam.1.3, "RData/GAM_1_3.rds")
} else {
  gam.1.3 <- readRDS("RData/GAM_1_3.rds")
}

summary(gam.1.3)

```

```{r}
plot(gam.1.3)
```

Not much variation in DOY among years. 

<!-- Is it related to ENSO events? Extract MEI.v2 values (https://psl.noaa.gov/enso/mei/) -->

<!-- ```{r} -->
<!-- MEI.data <- read.delim(file = "~/Oceans and Maps/Ocenographic data/meiv2_2022-05-18.txt", header = F, -->
<!--                        sep="", -->
<!--                        col.names = c("Year", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) %>% -->
<!--   pivot_longer(cols = "Jan":"Dec", names_to = "Month", values_to = "MEI") -->

<!-- MEI.data %>% filter(Year %in% years) %>% -->
<!--   group_by(Year) %>% -->
<!--   summarize(MEI.yr = sum(MEI)) -> MEI.data.yr -->

<!-- MEI.data %>%  -->
<!--   filter(Year %in% (years-1)) %>% -->
<!--   group_by(Year) %>% -->
<!--   summarize(MEI.yr.1 = sum(MEI)) %>% -->
<!--   mutate(Year = as.numeric(Year) + 1) -> MEI.data.yr.lag.1 -->

<!-- count.long.df %>% left_join(MEI.data.yr, by = "Year") -> count.long.df -->
<!-- count.long.df %>% left_join(MEI.data.yr.lag.1, by = "Year") -> count.long.df   -->


<!-- gam.2 <- gam(Count ~ s(Effort) + s(DOY) + s(Year) +  -->
<!--                s(Week) + s(MEI.yr) + s(MEI.yr.1),  -->
<!--              data = count.long.df) -->
<!-- ``` -->

<!-- That was a bit of digression. They don't add much to explaining deviance. Don't bother it for now. Also, this won't help with filling in the missing data points. -->


Try zero-inflated Poisson.

```{r gam.2, cache=T}

if (!file.exists("RData/GAM_2.rds")){
 
  gam.2 <- gam(Mother_Calf ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
               data = count.long.df, family = "ziP")
  saveRDS(gam.2, "RData/GAM_2.rds")
} else {
  gam.2 <- readRDS("RData/GAM_2.rds")
}
summary(gam.2)
```


```{r}
pred.year.gam.2 <- tidygam::predict_gam(model = gam.2, 
                               series = "Year",
                               length_out = 100,
                               tran_fun = exp)
plot(pred.year.gam.2)

# tidymv::plot_smooths(model = gam.2, 
#              series = Year, 
#              transform = exp)
```

Tweedie seems to fit better. 

Try negative binomial. 

```{r gam.3, cache=T}

if (!file.exists("RData/GAM_3.rds")){
 
  gam.3 <- gam(Mother_Calf ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
               data = count.long.df, family = "nb")
  saveRDS(gam.3, "RData/GAM_3.rds")
} else {
  gam.3 <- readRDS("RData/GAM_3.rds")
}

summary(gam.3)
```

Negative binomial seems to fit even better. >77% of deviance explained. 


```{r}

pred.gam.3.effort <- tidygam::predict_gam(model = gam.3,
                                          length = 200,
                                          series = "Effort")

plot(pred.gam.3.effort)
```


```{r}

pred.gam.3.DOY <- tidygam::predict_gam(model = gam.3,
                                          length = 200,
                                          series = "DOY")

plot(pred.gam.3.DOY)

```



```{r}

pred.gam.3.year <- tidygam::predict_gam(model = gam.3,
                                          length = 200,
                                          series = "Year")

plot(pred.gam.3.year)


```




```{r}

pred.gam.3.week <- tidygam::predict_gam(model = gam.3,
                                          length = 200,
                                          series = "Week")

plot(pred.gam.3.week)

```




```{r gam.3.1, cache=T}
if (!file.exists("RData/GAM_3_1.rds")){
 
  gam.3.1 <- gam(Mother_Calf ~ s(Effort) + s(DOY, Year) + s(Week), 
                 data = count.long.df, family = "nb")
  saveRDS(gam.3.1, "RData/GAM_3_1.rds")
} else {
  gam.3.1 <- readRDS("RData/GAM_3_1.rds")
}
summary(gam.3.1)
```



```{r gam.3.2, cache=T}
if (!file.exists("RData/GAM_3_2.rds")){
  
  gam.3.2 <- gam(Mother_Calf ~ s(Year) + s(Effort) + s(DOY, by = Year.f) + s(Week), 
                 data = count.long.df, 
                 family = "nb")
  saveRDS(gam.3.2, "RData/GAM_3_2.rds")
} else {
  gam.3.2 <- readRDS("RData/GAM_3_2.rds")
}
summary(gam.3.2)
```


```{r gam.4, cache=T}
if (!file.exists("RData/GAM_4.rds")){
  
  gam.4 <- gam((Mother_Calf+1) ~ s(Effort) + s(DOY) + s(Year) + s(Week), 
             data = count.long.df, family = "poisson")
  saveRDS(gam.4, "RData/GAM_4.rds")
} else {
  gam.4 <- readRDS("RData/GAM_4.rds")
}

summary(gam.4)
```

Negative binomial seems to be the best. 

```{r}
pred.gam.3.2.effort <- tidygam::predict_gam(model = gam.3.2,
                                            series = "Effort",
                                            length_out = 200)
# plot_smooths(model = gam.3,
#              series = Effort,
#              transform = exp)
plot(pred.gam.3.2.effort)
```


```{r}
pred.gam.3.2.DOY <- tidygam::predict_gam(model = gam.3.2,
                                            series = "DOY",
                                            length_out = 200)

plot(pred.gam.3.2.DOY)

```



```{r}

pred.gam.3.2.week <- tidygam::predict_gam(model = gam.3.2,
                                            series = "Week",
                                            length_out = 200)

plot(pred.gam.3.2.week)


```


```{r}

pred.gam.3.2.year <- tidygam::predict_gam(model = gam.3.2,
                                            series = "Year",
                                            length_out = 200)

plot(pred.gam.3.2.year)


```

The cyclical nature of the year effect is interesting... 

Effects of DOY, Year, and Week seem to be quite small... 

Add Beaufort and visibility to the best model

```{r gam.3.3, cache=T}
if (!file.exists("RData/GAM_3_3.rds")){
  
  gam.3.3 <- gam(Mother_Calf ~ s(Year) + s(Effort) + s(DOY) + s(Week) + s(BF, k = 3) + s(VS, k = 3), 
                 data = count.long.df %>% filter(BF < 8), 
                 family = "nb")
  saveRDS(gam.3.3, "RData/GAM_3_3.rds")
} else {
  gam.3.3 <- readRDS("RData/GAM_3_3.rds")
}
summary(gam.3.3)
```

```{r}
pred.gam.3.3.Beaufort <- tidygam::predict_gam(model = gam.3.3,
                                            series = "BF",
                                            length_out = 5)

plot(pred.gam.3.3.Beaufort)
```


```{r}
pred.gam.3.3.Vis <- tidygam::predict_gam(model = gam.3.3,
                                            series = "VS",
                                            length_out = 5)

plot(pred.gam.3.3.Vis)
```


prediction using the gam model
```{r}
pred.data <- count.long.df %>% 
  mutate(Effort = 3)

gam.3.2.pred <- predict.gam(gam.3.2,
                          type = "response",
                          se.fit = T)

pred.data %>% 
  mutate(prediction = gam.3.2.pred$fit,
         LCL = gam.3.2.pred$fit - gam.3.2.pred$se.fit * 1.96,
         UCL = gam.3.2.pred$fit + gam.3.2.pred$se.fit * 1.96) -> pred.data.3.2

ggplot(data = pred.data.3.2) +
  geom_point(aes(x = Mother_Calf, 
                 y = prediction)) +
  geom_errorbar(aes(x = Mother_Calf, ymin = LCL, ymax = UCL))
```


It does pretty good job for counts that are less than 5, but larger values fail pretty badly. The model fit is okay just because of that. But, the larger values are as important, even though they don't happen too often... 

```{r}
gam.3.1.pred <- predict.gam(gam.3.1,
                          type = "response",
                          se.fit = T)

pred.data %>% 
  mutate(prediction = gam.3.1.pred$fit,
         LCL = gam.3.1.pred$fit - gam.3.1.pred$se.fit * 1.96,
         UCL = gam.3.1.pred$fit + gam.3.1.pred$se.fit * 1.96) -> pred.data.3.1

ggplot(data = pred.data.3.1) +
  geom_point(aes(x = Mother_Calf, 
                 y = prediction)) +
  geom_errorbar(aes(x = Mother_Calf, ymin = LCL, ymax = UCL))
```


I need to build a year-specific model to fill in the gaps. 

Back to year-specific models for estimating annual calf production. Long term analyses will be done separately. I looked at how the binomial likelihood for observed counts fit to the data. It turned out the binomial model had high Pareto k values. Taking the 2022 season for example...

```{r}
library("loo")
results.2022.v1 <- readRDS(file = "RData/calf_estimates_v1_2022.rds")

pareto_k_table(results.2022.v1$MCMC.diag$loo.out)

```

v1 is the following:

model {
  p.obs ~ dnorm(0.889,0.06375^-2) #estimated observation probability

  for(j in 1:n.weeks){
    lambda[j] ~ dunif(0,40)
  }

  for(i in 1:n.obs){
    count.true[i] ~ dpois(lambda[week[i]])
    # watch period is 3 hours, so any amount of effort 
    # below 3 hours scales the observation probability accordingly
    p.obs.corr[i] <- p.obs * effort[i]/3 
        
    count.obs[i] ~ dbin(p.obs.corr[i], count.true[i])
    loglik[i] <- logdensity.bin(count.obs[i], p.obs.corr[i], count.true[i])
  }#i

  Total.Calves <- sum(count.true[1:n.obs])

}#model
    
lambda is an IID random deviate over n.weeks weeks. In reality, this changes in somewhat expected ways: increase, then decrease. Also, two consecutive shifts may be auto-correlated. 


```{r}
results.v2 <- readRDS(file = "RData/calf_estimates_v2.rds")

pareto_k_table(results.v2$MCMC.diag$loo.out)

```

V2 is quite bad in goodness-of-fit, according to Pareto k diagnostic values.

v2 has year-week specific lambdas, rather than IID over all weeks.

model {
  p.obs ~ dnorm(0.889,0.06375^-2) #estimated observation probability

  for (y in 1:n.years){
    for(j in 1:n.weeks[y]){
      lambda[y,j] ~ dunif(0,40) # dgamma(2, 0.15) 
    } #j
      
    for(i in 1:n.obs[y]){
      count.true[y,i] ~ dpois(lambda[y, week[y,i]])

      # watch period is 3 hours, so any amount of effort 
      # below 3 hours scales the observation probability accordingly:      
      p.obs.corr[y,i] <- p.obs * effort[y,i]/3
        
      count.obs[y,i] ~ dbin(p.obs.corr[y, i], count.true[y, i])
      loglik[y,i] <- logdensity.bin(count.obs[y,i], p.obs.corr[y,i], count.true[y,i])
    }#i

    Total.Calves[y] <- sum(count.true[y, 1:n.obs[y]])

  }#y
      
}#model
    

```{r}
results.2022.v3 <- readRDS(file = "RData/calf_estimates_v3_2022.rds")

pareto_k_table(results.2022.v3$MCMC.diag$loo.out)
```

v3 is similar to v1 but uses Poisson rather than binomial for the likelihood function. 

model {
  p.obs ~ dnorm(0.889,0.06375^-2) #estimated observation probability

  for(j in 1:n.weeks){
    lambda[j] ~ dunif(0,2000)
    count.true[j] ~ dpois(lambda[j]) 
        
    p.obs.corr[j] <- p.obs * prop.effort[j]

    count.obs[j] ~ dpois(p.obs.corr[j] * count.true[j])
    loglik[j] <- logdensity.pois(count.obs[j], p.obs.corr[j] * count.true[j])
  }

  Total.Calves <- sum(count.true)

}#model


```{r}
results.2022.v4 <- readRDS(file = "RData/calf_estimates_v4_2022.rds")

pareto_k_table(results.2022.v4$MCMC.diag$loo.out)
```

v4 uses week-specific uniform lambda prior and binomial likelihood but Poisson prior on the true number of mother-calf pairs. 

model {
  p.obs ~ dnorm(0.889,0.06375^-2) #estimated observation probability

  for(j in 1:n.weeks){
    lambda[j] ~ dunif(0,2000)
    count.true[j] ~ dpois(lambda[j]) 
        
    p.obs.corr[j] <- p.obs * prop.effort[j]
    count.obs[j] ~ dbin(p.obs.corr[j], count.true[j])
    loglik[j] <- logdensity.bin(count.obs[j], p.obs.corr[j], count.true[j])

  }

  Total.Calves <- sum(count.true)

}#model
    
    

```{r}
results.2022.v5 <- readRDS(file = "RData/calf_estimates_v5_2022.rds")

pareto_k_table(results.2022.v5$MCMC.diag$loo.out)
```

v5 has week-specific lambda, just as in v1. The likelihood function is Poisson. 


model {
  p.obs ~ dnorm(0.889,0.06375^-2) #estimated observation probability

  for(j in 1:n.weeks){
    lambda[j] ~ dunif(0,40)
  }

  for(i in 1:n.obs){
    count.true[i] ~ dpois(lambda[week[i]])
    
    # watch period is 3 hours, so any amount of 
    # effort below 3 hours scales the observation probability accordingly
    p.obs.corr[i] <- p.obs * effort[i]/3 
        
    count.obs[i] ~ dpois(p.obs.corr[i] * count.true[i])
    loglik[i] <- logdensity.pois(count.obs[i], p.obs.corr[i] * count.true[i])
  }#i

  Total.Calves <- sum(count.true[1:n.obs])

}#model

    
    
```{r}
results.2022.v6 <- readRDS(file = "RData/calf_estimates_v6_2022.rds")

pareto_k_table(results.2022.v6$MCMC.diag$loo.out)
```

None of these models fit well... I think we need to make an auto-regressive model... 




